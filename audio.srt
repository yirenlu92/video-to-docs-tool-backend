1
00:00:00,320 --> 00:00:05,520
Thanks so much for arranging this and we're excited to be part of this summit and share the

2
00:00:05,520 --> 00:00:10,400
learnings and the platforms that we've led to. Let me try to see what app first.

3
00:00:12,400 --> 00:00:18,000
It works. I'm Srihar Shah and I led the data quality and data platforms for the wallet Uber

4
00:00:18,800 --> 00:00:25,040
and currently working on OpenMatera data. With that, I want to start off how did we arrive

5
00:00:25,040 --> 00:00:29,760
this situation, you know, necessarily investing data quality and better data culture at Uber.

6
00:00:30,960 --> 00:00:35,600
Uber kind of religion is the data transportation by connecting by covering millions of

7
00:00:35,600 --> 00:00:41,840
bytes, connecting riders, drivers, restaurants, each order, whatnot. Behind all of this

8
00:00:41,840 --> 00:00:48,000
transportation platform is big data and the data science. And we build really interesting services

9
00:00:48,000 --> 00:00:53,680
that so this particular picture kind of paints it what we do at a high level. So I'm going to go

10
00:00:53,680 --> 00:00:58,000
through that and you know go to the next steps on my DPP build that quality and data

11
00:00:58,000 --> 00:01:03,040
absolutely platforms it over. So if you look at the the left side we have a key value

12
00:01:03,040 --> 00:01:09,120
device which is nothing but a shorted mice equal where it all our mobile apps and back-and-service

13
00:01:09,120 --> 00:01:14,960
a slancer data that's our online database. From there we build an ingestion framework which

14
00:01:14,960 --> 00:01:22,160
takes the concepts of data lake and lands our online DPP data through CDC into data lake which is

15
00:01:22,160 --> 00:01:29,840
bought by high. So when this process finishes it runs 24 hours it's called raw tables. So it's a

16
00:01:29,840 --> 00:01:34,960
raw data that is getting captured from online tables. Then we have data warehouse team which takes

17
00:01:34,960 --> 00:01:40,000
the raw tables and builds more you know dimensional tables and packed tables kind of a typical

18
00:01:40,000 --> 00:01:45,280
across the industry what happens. And on top of that we build really nice utilities for the

19
00:01:46,080 --> 00:01:50,960
data science and engineering teams and our patients team to utilize this data. So the great thing

20
00:01:50,960 --> 00:01:55,200
about self-service teams is self-saving tools is you know it reduces the load on the engineering

21
00:01:55,200 --> 00:02:00,800
team it gives a lot of power to our users but also though it also generates a lot of data for us.

22
00:02:00,800 --> 00:02:08,480
So as the business crew we end up you know growing data lake as well by the user page and this

23
00:02:08,480 --> 00:02:14,880
actually caused a lot of issues at a height of this platform we have you know 300k data sets.

24
00:02:14,880 --> 00:02:21,280
And the issues are data duplication because no one knew what data existed and even if they can

25
00:02:21,280 --> 00:02:26,880
discard the data they don't know who to talk to and you know understand the data so they end up

26
00:02:26,880 --> 00:02:33,600
kind of creating their own siloed version of data again and again. Right so during this years we

27
00:02:33,600 --> 00:02:37,840
consider mostly on let's say you know scaling that in front of itself but kind of lacked

28
00:02:37,840 --> 00:02:44,560
investment towards the data product itself. So as I said like you know data duplications

29
00:02:44,560 --> 00:02:49,360
data discovery became a problem and so is the data quality. So nobody knew if the data is

30
00:02:49,360 --> 00:02:54,480
kind of landing in these tables even if they didn't land and it's getting covered by dashboards

31
00:02:54,480 --> 00:03:00,960
which are not only used by several data scientists users, executives and also you know external

32
00:03:00,960 --> 00:03:05,680
parties to understand what's going at Uber and how to better improve these products. We couldn't

33
00:03:05,680 --> 00:03:11,600
tell like no if the quality of the data that is landing there is great. Right so from there we

34
00:03:11,600 --> 00:03:20,480
applied some of the first principles to evaluate a better data culture. So as I said like at the

35
00:03:20,480 --> 00:03:26,400
height of the data platform we have a 300k data sets by taking an of a fundamental approach to

36
00:03:26,400 --> 00:03:31,520
and kind of looking at how to better the data at Uber with a just quite a bit of that from

37
00:03:31,520 --> 00:03:37,280
under-sub pipelines to that if they get pipelines from 300k data set which are not owned nobody

38
00:03:37,280 --> 00:03:43,040
can have knew who is the owner of those assets to 130k data sets which are owned and we have

39
00:03:43,040 --> 00:03:48,720
them you know around 10k weekly active users and around the 4,000 metrics that are following this data

40
00:03:48,720 --> 00:03:56,960
itself. So let me going to what are the data for for sprints data as a core. What that exactly means

41
00:03:56,960 --> 00:04:02,320
is you know when data is getting created and deeply created are being changed it has to go

42
00:04:02,320 --> 00:04:07,920
through certain process. So schema change and detections and everything are great but schema change

43
00:04:07,920 --> 00:04:13,600
notifications can only address the problem after it happens right. You have a producer who created

44
00:04:13,600 --> 00:04:18,000
a table and they changed the name of the column are changed the type of the column then that's a

45
00:04:18,000 --> 00:04:23,440
backward incompatible change and they landed in production and your pipelines are broken by then

46
00:04:23,440 --> 00:04:28,400
you get a seam notification but the problem is already you know happened and you need to go back

47
00:04:28,400 --> 00:04:34,160
and inverse lot of engineering else to fix that. So by creating data as a code that data artifacts

48
00:04:34,160 --> 00:04:40,480
has to be properly reviewed so any schema change that we do in production go through review process.

49
00:04:40,480 --> 00:04:45,760
This review process not only tags the the producers of the data but also anyone who's consuming

50
00:04:45,760 --> 00:04:52,880
the data itself right so that's part of the data as a code. Now data is owned similar to what

51
00:04:52,880 --> 00:04:57,920
services are when you actually building a service let's say Kafka for example you're putting it

52
00:04:57,920 --> 00:05:04,240
in service in production it is owned by a team a group of people that make sure that

53
00:05:04,240 --> 00:05:08,880
I you think the service is up and running similarly when you're creating a data table it should

54
00:05:08,880 --> 00:05:13,680
have an owner association a team should own that they need to clearly define what is the

55
00:05:14,720 --> 00:05:19,680
what is the intent of the data product itself artifact itself and own it to provide

56
00:05:19,680 --> 00:05:26,120
a guarantee surround the data and finally data quality is known so data quality here is not just

57
00:05:26,120 --> 00:05:33,120
about anomaly directions are tested itself but overall SLAs around the data artifact you as

58
00:05:33,120 --> 00:05:38,720
the owner should provide what is a SLA of a data artifact that you created and provide SLAs not

59
00:05:38,720 --> 00:05:44,000
just for the data quality but also provide SLAs around you know any bugs being filed on your data

60
00:05:44,000 --> 00:05:50,320
artifact and any instance being filed around your data artifact how soon you can rectify that

61
00:05:50,320 --> 00:05:56,320
and who is responsible for that so from a such a platform where you know data is kind of

62
00:05:56,320 --> 00:06:00,320
well-versed you know the self-serving tools anyone can go and play around the data and play

63
00:06:00,320 --> 00:06:07,120
their own dashboard again their own data assets to a much more regulated wound and responsible

64
00:06:07,120 --> 00:06:12,800
data platform that we build and we can transform the data culture to work so these are kind of a

65
00:06:12,800 --> 00:06:19,920
foundational principle that we started working on and enforcing it so we next slide

66
00:06:22,880 --> 00:06:27,920
and among 132 data sets and you cannot or 300 data sets you cannot kind of assume all of

67
00:06:27,920 --> 00:06:35,040
that data is important right so similar to we services at Uber are extra syndicative that you

68
00:06:35,040 --> 00:06:42,160
can observe there is something called a steer concept example of Uber let's say Kafka as a

69
00:06:42,160 --> 00:06:46,480
service tiered as zero because it is extremely important to business any

70
00:06:47,200 --> 00:06:52,240
reliability concerns in Kafka impacts not just the business but also user experience

71
00:06:52,240 --> 00:06:56,160
similarly we have other services high when other things which are also extremely important but

72
00:06:56,160 --> 00:07:02,160
doesn't invite you using experience that much so we call that STL1 right so when you're building

73
00:07:02,160 --> 00:07:07,280
the data artifacts as well you know to that out that you are kind of building and providing

74
00:07:07,280 --> 00:07:12,560
as a service for the rest of the company has certain importance to it like is this data asset is

75
00:07:12,560 --> 00:07:17,440
being used to make patients around the business is this used to make you know production

76
00:07:18,080 --> 00:07:23,200
product related issues are any among models in real time are depending on this data asset

77
00:07:23,200 --> 00:07:30,160
so a tier one is an extremely important data asset and a tier five for example is an individually

78
00:07:30,160 --> 00:07:35,200
owned that generating a temporary staging tables which will you know doesn't have any guarantees

79
00:07:35,200 --> 00:07:40,480
at all so we brought in the tiering concept we tiered pretty much all the data assets at Uber

80
00:07:40,480 --> 00:07:47,360
including not just the tables pipelines ML models and dashboards as well so with that we came from

81
00:07:47,360 --> 00:07:54,640
300k data tables to 20k 100 tier one tier two tables that are extremely important so then you

82
00:07:54,640 --> 00:07:59,200
know what is extremely important in your business you can actually focus on the important data assets

83
00:07:59,840 --> 00:08:04,880
and not just the focus but you can run the queries through that how to get better resource for

84
00:08:04,880 --> 00:08:12,160
those tables so and so and this code is another problem kind of cost the duplication at Uber as well

85
00:08:12,880 --> 00:08:18,080
fundamentally kind of address that through providing not just a better catalog but better search

86
00:08:18,080 --> 00:08:25,840
capabilities and another problem that I kind of want to talk about in this case is the ILO

87
00:08:25,840 --> 00:08:31,520
tooling that we had before and after the issues so whenever we actually look at these data problems

88
00:08:31,520 --> 00:08:36,160
each individual team at Uber kind of develop their own cello tools you know someone has a

89
00:08:36,160 --> 00:08:41,120
disparity problem they build a catalog someone has a quality problem anomaly detection they only

90
00:08:41,120 --> 00:08:47,440
build those specific use cases and what end up happening is metadata can have divided

91
00:08:47,440 --> 00:08:52,240
in each of these tools because kind of metadata is a conditional tool and any of these tooling

92
00:08:52,240 --> 00:08:59,520
they end up working the metadata itself there is no centralized ones to store for metadata itself

93
00:08:59,520 --> 00:09:04,720
when we are addressing this problem we build what we call as humater data a foundational piece for

94
00:09:04,720 --> 00:09:10,800
all the metadata related activities so we build crawlers and build metadata into humater data on

95
00:09:10,800 --> 00:09:17,120
top of that we enable all these use cases so if you go to the data book although it talks about

96
00:09:17,120 --> 00:09:22,000
cataloging in discovery but you find all the important information details in the data book

97
00:09:22,000 --> 00:09:28,240
data book is kind of a UI on top of humater data so you can actually go look at what is your quality

98
00:09:28,240 --> 00:09:36,240
signals what is a tier who own these particular data assets lineage and you know entire product

99
00:09:36,240 --> 00:09:41,520
itself that enable by the humater data and the data book itself and the next part is lineage

100
00:09:42,640 --> 00:09:47,440
as previous speakers spoke lineage is extremely important not just for kind of you are understanding

101
00:09:47,440 --> 00:09:52,160
of how the data flows are the governance model but lineage is extremely important in the data quality

102
00:09:52,160 --> 00:09:57,360
enforcement as well you will talk about in next few slides and the final part of the equation is

103
00:09:57,360 --> 00:10:01,600
what are the guarantees that we are giving out of the data book especially tier one tier two data

104
00:10:01,600 --> 00:10:08,480
sets which we said like my extremely important business so when we when a data asset gets onto

105
00:10:08,480 --> 00:10:14,080
tier one tier two there are certain tools to follow for that but when they qualify for tier one tier

106
00:10:14,080 --> 00:10:19,840
two we automatically unbode them into our data quality and enforce certain checks and certain

107
00:10:19,840 --> 00:10:25,200
foundation principles if you are tier one tier two your data artifact should be documented you

108
00:10:25,200 --> 00:10:30,640
should be owned there should be a major duty uncle should be associated with tier one tier two

109
00:10:30,640 --> 00:10:37,440
data assets and any alerts any instance will automatically will you know trigger and alert

110
00:10:37,440 --> 00:10:42,640
towards the team that owns so if you think of that is essentially a at least at that point when

111
00:10:42,640 --> 00:10:47,760
you're like during the data assets so those are kind of a high level overview of you know what

112
00:10:47,760 --> 00:10:52,400
state I it over what are the problems that we encountered and how did we kind of fundamentally

113
00:10:52,400 --> 00:10:58,800
address the data quality or the data ownership itself so in the next few slides Sange will go

114
00:10:58,800 --> 00:11:02,960
through more details about the data quality itself and kind of our picture details of that

115
00:11:03,920 --> 00:11:05,760
with that Sange you want to take over

116
00:11:12,240 --> 00:11:18,400
sorry I was on mute yeah so as Harsha was saying right the two fundamental pillars for data

117
00:11:18,400 --> 00:11:24,960
qualities the governance part and then the tools part the data book which is the metadata catalog

118
00:11:24,960 --> 00:11:32,560
the lineage right and data quality is kind of built on top of these two so and the most important

119
00:11:32,560 --> 00:11:37,200
thing is the tiding part we started with tearing all the data sets to find out how many of these

120
00:11:37,920 --> 00:11:46,640
dedicated assets are important and as you can see the number of tier data sets where at the beginning

121
00:11:46,640 --> 00:11:52,560
of the 2020 we didn't have a lot of data sets paired and then now almost all the data sets are

122
00:11:52,560 --> 00:11:59,280
tiered and it turns out that of the 130 data sets we had only around 3000 of them are tier 1 and tier 2

123
00:11:59,280 --> 00:12:04,880
so those were the ones that are going to impact the business so we could focus more on them so

124
00:12:04,880 --> 00:12:11,280
that that's a small number to work with it's also growing but it's it kind of separates out the

125
00:12:11,280 --> 00:12:19,520
noise there so once we have these things right and as Harsha said we set up the governance right

126
00:12:19,520 --> 00:12:24,160
and then we went behind all the table owners to make sure they documented data sets that is

127
00:12:24,160 --> 00:12:31,760
description they add their owners the owner should not be individual it should be teams and then

128
00:12:32,240 --> 00:12:36,400
they have the page duty setup right so people know who to talk to one of the biggest problem was

129
00:12:36,400 --> 00:12:41,440
they are working on this data set and something has changed and they don't know who to reach out to

130
00:12:41,440 --> 00:12:47,440
because there are a lot of people you know worked on this over a period of time right so we make

131
00:12:47,440 --> 00:12:52,560
sure teams own the data sets and there is a priority to reach out to them and as you can see in

132
00:12:52,560 --> 00:12:57,680
the slide there is also freshness duplicates and cross DC consistency checks I'll come to that

133
00:12:58,400 --> 00:13:06,160
later so in the next slide so then we had a system first of which we contrast okay where

134
00:13:06,160 --> 00:13:12,560
people were able to come and define their own checks right so what happened was teams that had

135
00:13:12,560 --> 00:13:17,760
really good monitoring practices right they had a lot of checks some people teams did not have checks

136
00:13:18,640 --> 00:13:21,200
at our level we were not able to answer the question is this check enough is this check not enough which tables are covered which are not covered which are so that was a big problem for us

137
00:13:21,200 --> 00:13:26,320
so we have a lot of conversation around that and we decided on four major categories of checks

138
00:13:26,320 --> 00:13:35,360
right and one is freshness and in some places it's also called as completeness and the idea here is

139
00:13:35,360 --> 00:13:42,720
let's say you had a trip that happened at 12 pm right and then it flowed through the

140
00:13:42,720 --> 00:14:05,720
So called as completeness and the idea here is, let's say you had a trip that happened at 12 pm right and then it flows through a Kafka topic and then it's in addition into the raw data table at 1 pm and then there are there is a model table and that is getting calculated at 3 pm. When the table runs when the pipeline turns at 3 pm, the freshness is actually 3 hours right at a high level that's how we measure the freshness to latency for the data to travel to the consumption point and there was a very important metric because there some teams might have it offered pretty for our freshness others might offer 48 freshness but when you are going as a downstream consumer right when when you have a dashboard or a

141
00:14:42,720 --> 00:15:04,720
But what we do is whatever SLA is being set by the owner right we hold them accountable by by making sure we constantly monitoring these freshness right the second part is completeness completeness kind of you know tied with freshness so it's not just how soon the data is coming is the data complete right you know you process the data is coming right.

142
00:15:04,720 --> 00:15:21,720
So you process the partition for yesterday but the partition for day before yesterday is incomplete right there was an incident the pipeline broke and the data is still missing right so completeness gives make sure that all the data that is present in upstream is present in downstream.

143
00:15:21,720 --> 00:15:45,720
So for us DC Uber operates in two major physical regions and all the data is backed up in the other region for higher availability and durability right and the customers they don't always consume the data from one region they are kind of distributed across the regions so we want to make sure that the data is consistent at the physical level on both these locations.

144
00:15:45,720 --> 00:16:07,720
So that as a consumer of this data right you should not be worried about the physical location of the data right you are free to consume it anywhere and duplicates is a lot of data we assume the records only just once right simple example could be user records that is only one user account so

145
00:16:07,720 --> 00:16:31,720
the pipelines kind of assume that and they have it so in my databases like my secret right once you have a primary key you can't even insert the duplicate record but but with big data right that's not possible to prevent before insertion so what we do is after insertion keep checking for duplicates and it's based on the primary keys that is defined in the

146
00:16:31,720 --> 00:16:56,720
metadata catalog right now these are the basic checks that we define but on top of it we also allow users to write their own checks the own checks can be a custom query checking a particular thing because the fifth pillar that is not that is not mentioned is the semantic completeness of the data so it's the owners who know

147
00:16:56,720 --> 00:17:22,720
that the consumer so maybe you're looking at certain distribution of a column it's an integer column right and let's say it's the trip among right it doesn't make sense to be a negative value or it doesn't make sense to cost millions of dollars right so you could have a check on the distribution of the trip among and it could be specific to a certain region right so we allow users to define their own checks something that we cannot

148
00:17:22,720 --> 00:17:52,680
enforce at an org level for all the tables right because the context is really important here and then they choose their schedule and we they can go to the same data book that and they can define those checks right once all these checks are on board it right they show up along with the table so anybody who's coming to data book searching for a data right the same place that gives them the ownership the same place that tells them the description of the data is the same place where the

149
00:17:52,680 --> 00:18:11,680
quality of the data is also visible so they can look at all these checks they can pick a time range that they are interested in they can dig in and see how these checks are doing right and and it's also propagated to all the downstream systems so

150
00:18:11,680 --> 00:18:41,640
we have our own ML platform right so whenever somebody is training their ML model along with the ML quality report the data quality report is also attached or if it's dashboard or it could be a ad hoc exploratory query right so you're a data scientist you want to build some new insights right so you're just going and running some queries and checking the data so in the query builder tool that we have we surface the data quality hey you're checking the stable and you're looking at these partitions but

151
00:18:41,640 --> 00:18:58,200
during this partition dates there are some data quality issues right and when you go and search when we show the summary of all the tables we show like a green to indicate healthy right or or to indicate the data quality is not fine right so

152
00:18:58,200 --> 00:19:14,680
and what's the impact right so we defined down times based on these checks so when the checks are met the data is good when the checks are not met the the time period is how the downtime is calculated similar to what the previous talk or

153
00:19:14,680 --> 00:19:33,100
modicalo they were saying so we come up with the sl and as you can see once we started our governance once we started the data quality right you can see our sl going up we're almost at double 9 snow our not stars to be triple 9s in terms of tier 1 and tier 2 sls we're not

154
00:19:33,100 --> 00:19:35,300
but we're kind of progressing there.

155
00:19:35,300 --> 00:19:37,300
Right.

156
00:19:37,300 --> 00:19:38,700
So what is the future for us?

157
00:19:38,700 --> 00:19:39,700
Right.

158
00:19:39,700 --> 00:19:44,300
So we want to enable more properties for the semantic test,

159
00:19:44,300 --> 00:19:46,300
multi-dimensional test.

160
00:19:46,300 --> 00:19:48,900
We also want to enable code reviews for tests.

161
00:19:48,900 --> 00:19:52,700
When users create the test, we want consumers

162
00:19:52,700 --> 00:19:57,100
can use the data quality platform as a SLA negotiations platform.

163
00:19:57,100 --> 00:19:59,100
So the consumers can go and create the test.

164
00:19:59,100 --> 00:20:01,100
The owners can review it and approve it.

165
00:20:01,100 --> 00:20:04,620
And the SLA, the thresholds of the test

166
00:20:04,620 --> 00:20:07,020
will page the owners right if it is not being met.

167
00:20:07,020 --> 00:20:12,820
And we are also building features like anomaly detection,

168
00:20:12,820 --> 00:20:14,580
which is pretty standard in the industry,

169
00:20:14,580 --> 00:20:18,780
and expose that as another offering as well.

170
00:20:18,780 --> 00:20:22,420
One final thing is something that Hachha is working on.

171
00:20:22,420 --> 00:20:23,420
It's open metadata.

172
00:20:23,420 --> 00:20:26,340
I would like Hachha to improve on that.

173
00:20:26,340 --> 00:20:26,860
Thanks, Anjai.

174
00:20:26,860 --> 00:20:29,180
So as I talked about all these issues,

175
00:20:29,180 --> 00:20:31,580
that's not unique to the Uber itself.

176
00:20:31,580 --> 00:20:33,700
These are the issues that are face-to-face industry.

177
00:20:33,700 --> 00:20:36,940
So with that, to make these learnings

178
00:20:36,940 --> 00:20:40,820
and kind of solutions that we worked on Uber and previous companies,

179
00:20:40,820 --> 00:20:43,660
we started a open metadata.org as a project.

180
00:20:43,660 --> 00:20:46,660
So if you're interested in solving these issues,

181
00:20:46,660 --> 00:20:48,300
want to take a look, please come and take a look

182
00:20:48,300 --> 00:20:51,300
at OpenSource project there.

183
00:20:51,300 --> 00:20:54,300
Thanks. Thank you.
